{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRFDEL14XTiN"
   },
   "source": [
    "# Agent Observability with OpenLIT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PqhddfHZXgVH"
   },
   "source": [
    "\n",
    "\n",
    "<img src=\"https://github.com/openlit/.github/blob/main/profile/assets/wide-logo-no-bg.png?raw=true\" alt=\"OpenLIT Logo\" width=\"30%\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmNFkiaGYXIv"
   },
   "source": [
    "[OpenLIT](https://github.com/openlit/openlit) an open source product that helps developers build and manage AI agents in production, effectively helping them improve accuracy. As a self-hosted solution, it enables developers to experiment with LLMs, manage and version prompts, securely manage API keys, and provide safeguards against prompt injection and jailbreak attempts. It also includes built-in OpenTelemetry-native observability and evaluation for the complete GenAI stack (LLMs, Agents, vector databases, and GPUs).\n",
    "\n",
    "For more info, check out the [OpenLIT Repo](https://github.com/openlit/openlit)\n",
    "\n",
    "![](https://github.com/openlit/.github/blob/main/profile/assets/openlit-client-1.png?raw=true)\n",
    "![](https://github.com/openlit/.github/blob/main/profile/assets/openlit-client-2.png?raw=true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hePjFDc6Yu6G"
   },
   "source": [
    "## Adding OpenLIT to an existing Autogen service\n",
    "To get started, you'll need to install the OpenLIT library\n",
    "\n",
    "OpenLIT uses OpenTelemetry to automatically intrument the AI Agent app when it's initialized meaning your agent observability data like execution traces and metrics will be tracked in just one line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "kyb8oFmCZdg8"
   },
   "outputs": [],
   "source": [
    "pip install ag2 openlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pxawniRGYyj2"
   },
   "outputs": [],
   "source": [
    "import openlit\n",
    "\n",
    "from autogen import AssistantAgent, UserProxyAgent\n",
    "\n",
    "openlit.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hK4KL2JLZwLg"
   },
   "source": [
    "OpenLIT will now start automatically tracking\n",
    "\n",
    "- LLM prompts and completions\n",
    "- Token usage and costs\n",
    "- Agent names and actions\n",
    "- Tool usage\n",
    "- Errors\n",
    "\n",
    "\n",
    "## Lets look at a simple chat example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Z_5QJzEqa7I0"
   },
   "outputs": [],
   "source": [
    "import openlit\n",
    "\n",
    "openlit.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2CKZPle2aal6",
    "outputId": "7f3346e9-7e5b-4c47-f111-32f8d06313a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_proxy (to assistant):\n",
      "\n",
      "Tell me a joke about NVDA and TESLA stock prices.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "assistant (to user_proxy):\n",
      "\n",
      "Why don't NVDA and TESLA stock prices ever get coffee together?\n",
      "\n",
      "Because whenever they heat up, they always take a steep drop before they can cool down! \n",
      "\n",
      "I hope this brings a smile to your face. Investing in stocks can be a rollercoaster sometimes. Please note that this is humor and doesn't reflect the actual dynamics of these companies' stock prices. TERMINATE.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Replying as user_proxy. Provide feedback to assistant. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: hi\n",
      "user_proxy (to assistant):\n",
      "\n",
      "hi\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "{\n",
      "    \"name\": \"openai.chat.completions\",\n",
      "    \"context\": {\n",
      "        \"trace_id\": \"0x35ae5952626492e432ae25af5bf92daa\",\n",
      "        \"span_id\": \"0x44103383aa51d1b1\",\n",
      "        \"trace_state\": \"[]\"\n",
      "    },\n",
      "    \"kind\": \"SpanKind.CLIENT\",\n",
      "    \"parent_id\": null,\n",
      "    \"start_time\": \"2024-11-21T01:53:47.597241Z\",\n",
      "    \"end_time\": \"2024-11-21T01:53:48.506758Z\",\n",
      "    \"status\": {\n",
      "        \"status_code\": \"OK\"\n",
      "    },\n",
      "    \"attributes\": {\n",
      "        \"telemetry.sdk.name\": \"openlit\",\n",
      "        \"gen_ai.system\": \"openai\",\n",
      "        \"gen_ai.operation.name\": \"chat\",\n",
      "        \"gen_ai.endpoint\": \"openai.chat.completions\",\n",
      "        \"gen_ai.response.id\": \"chatcmpl-AVqg7QSYB8CpEN1I5PYT1laBcds9S\",\n",
      "        \"gen_ai.environment\": \"default\",\n",
      "        \"gen_ai.application_name\": \"default\",\n",
      "        \"gen_ai.request.model\": \"gpt-4\",\n",
      "        \"gen_ai.request.top_p\": 1.0,\n",
      "        \"gen_ai.request.max_tokens\": -1,\n",
      "        \"gen_ai.request.user\": \"\",\n",
      "        \"gen_ai.request.temperature\": 1.0,\n",
      "        \"gen_ai.request.presence_penalty\": 0.0,\n",
      "        \"gen_ai.request.frequency_penalty\": 0.0,\n",
      "        \"gen_ai.request.seed\": \"\",\n",
      "        \"gen_ai.request.is_stream\": false,\n",
      "        \"gen_ai.usage.input_tokens\": 580,\n",
      "        \"gen_ai.usage.output_tokens\": 9,\n",
      "        \"gen_ai.usage.total_tokens\": 589,\n",
      "        \"gen_ai.response.finish_reasons\": [\n",
      "            \"stop\"\n",
      "        ],\n",
      "        \"gen_ai.usage.cost\": 0.017939999999999998\n",
      "    },\n",
      "    \"events\": [\n",
      "        {\n",
      "            \"name\": \"gen_ai.content.prompt\",\n",
      "            \"timestamp\": \"2024-11-21T01:53:48.506257Z\",\n",
      "            \"attributes\": {\n",
      "                \"gen_ai.prompt\": \"system: You are a helpful AI assistant.\\nSolve tasks using your coding and language skills.\\nIn the following cases, suggest python code (in a python coding block) or shell script (in a sh coding block) for the user to execute.\\n    1. When you need to collect info, use the code to output the info you need, for example, browse or search the web, download/read a file, print the content of a webpage or a file, get the current date/time, check the operating system. After sufficient info is printed and the task is ready to be solved based on your language skill, you can solve the task by yourself.\\n    2. When you need to perform some task with code, use the code to perform the task and output the result. Finish the task smartly.\\nSolve the task step by step if you need to. If a plan is not provided, explain your plan first. Be clear which step uses code, and which step uses your language skill.\\nWhen using code, you must indicate the script type in the code block. The user cannot provide any other feedback or perform any other action beyond executing the code you suggest. The user can't modify your code. So do not suggest incomplete code which requires users to modify. Don't use a code block if it's not intended to be executed by the user.\\nIf you want the user to save the code in a file before executing it, put # filename: <filename> inside the code block as the first line. Don't include multiple code blocks in one response. Do not ask users to copy and paste the result. Instead, use 'print' function for the output when relevant. Check the execution result returned by the user.\\nIf the result indicates there is an error, fix the error and output the code again. Suggest the full code instead of partial code or code changes. If the error can't be fixed or if the task is not solved even after the code is executed successfully, analyze the problem, revisit your assumption, collect additional info you need, and think of a different approach to try.\\nWhen you find an answer, verify the answer carefully. Include verifiable evidence in your response if possible.\\nReply \\\"TERMINATE\\\" in the end when everything is done.\\n    \\nuser: Tell me a joke about NVDA and TESLA stock prices.\\nassistant: Why don't NVDA and TESLA stock prices ever get coffee together?\\n\\nBecause whenever they heat up, they always take a steep drop before they can cool down! \\n\\nI hope this brings a smile to your face. Investing in stocks can be a rollercoaster sometimes. Please note that this is humor and doesn't reflect the actual dynamics of these companies' stock prices. TERMINATE.\\nuser: hi\"\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"gen_ai.content.completion\",\n",
      "            \"timestamp\": \"2024-11-21T01:53:48.506314Z\",\n",
      "            \"attributes\": {\n",
      "                \"gen_ai.completion\": \"Hello! How can I assist you today?\"\n",
      "            }\n",
      "        }\n",
      "    ],\n",
      "    \"links\": [],\n",
      "    \"resource\": {\n",
      "        \"attributes\": {\n",
      "            \"telemetry.sdk.language\": \"python\",\n",
      "            \"telemetry.sdk.name\": \"openlit\",\n",
      "            \"telemetry.sdk.version\": \"1.28.2\",\n",
      "            \"service.name\": \"default\",\n",
      "            \"deployment.environment\": \"default\"\n",
      "        },\n",
      "        \"schema_url\": \"\"\n",
      "    }\n",
      "}\n",
      "assistant (to user_proxy):\n",
      "\n",
      "Hello! How can I assist you today?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Replying as user_proxy. Provide feedback to assistant. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: exit\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatResult(chat_id=None, chat_history=[{'content': 'Tell me a joke about NVDA and TESLA stock prices.', 'role': 'assistant', 'name': 'user_proxy'}, {'content': \"Why don't NVDA and TESLA stock prices ever get coffee together?\\n\\nBecause whenever they heat up, they always take a steep drop before they can cool down! \\n\\nI hope this brings a smile to your face. Investing in stocks can be a rollercoaster sometimes. Please note that this is humor and doesn't reflect the actual dynamics of these companies' stock prices. TERMINATE.\", 'role': 'user', 'name': 'assistant'}, {'content': 'hi', 'role': 'assistant', 'name': 'user_proxy'}, {'content': 'Hello! How can I assist you today?', 'role': 'user', 'name': 'assistant'}], summary='Hello! How can I assist you today?', cost={'usage_including_cached_inference': {'total_cost': 0.03731999999999999, 'gpt-4-0613': {'cost': 0.03731999999999999, 'prompt_tokens': 1068, 'completion_tokens': 88, 'total_tokens': 1156}}, 'usage_excluding_cached_inference': {'total_cost': 0.017939999999999998, 'gpt-4-0613': {'cost': 0.017939999999999998, 'prompt_tokens': 580, 'completion_tokens': 9, 'total_tokens': 589}}}, human_input=['hi', 'exit'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from autogen import AssistantAgent, UserProxyAgent\n",
    "\n",
    "llm_config = {\"model\": \"gpt-4\", \"api_key\": os.environ[\"OPENAI_API_KEY\"]}\n",
    "assistant = AssistantAgent(\"assistant\", llm_config=llm_config)\n",
    "user_proxy = UserProxyAgent(\"user_proxy\", code_execution_config=False)\n",
    "\n",
    "# Start the chat\n",
    "user_proxy.initiate_chat(\n",
    "    assistant,\n",
    "    message=\"Tell me a joke about NVDA and TESLA stock prices.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7_gNWze0bYdO"
   },
   "source": [
    "If the `otlp_endpoint` or `OTEL_EXPORTER_OTLP_ENDPOINT` is not provided, the OpenLIT SDK will output traces and metrics directly to your console, which is recommended during the development phase.\n",
    "\n",
    "To send the traces and metrics to OpenLIT or any OpenTelemetry backend, Configure the telemetry data destination as follows:\n",
    "\n",
    "| Purpose                                   | Parameter/Environment Variable                   | For Sending to OpenLIT         |\n",
    "|-------------------------------------------|--------------------------------------------------|--------------------------------|\n",
    "| Send data to an HTTP OTLP endpoint        | `otlp_endpoint` or `OTEL_EXPORTER_OTLP_ENDPOINT` | `\"http://127.0.0.1:4318\"`      |\n",
    "| Authenticate telemetry backends           | `otlp_headers` or `OTEL_EXPORTER_OTLP_HEADERS`   | Not required by default        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHz5kq-qcxfM"
   },
   "source": [
    "![](https://github.com/openlit/.github/blob/main/profile/assets/openlit-ag2-1.png?raw=true)\n",
    "![](https://github.com/openlit/.github/blob/main/profile/assets/openlit-ag2-2.png?raw=true)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "front_matter": {
   "description": "Use OpenLIT to easily monitor AI agents in production with OpenTelemetry.",
   "tags": [
    "integration",
    "monitoring",
    "debugging"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
